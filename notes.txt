What I'm thinking is to create an Optimal Agent (VI) that runs on every proxy reward.

This would look like
Train process -->
Test model -->
Generate rewards -->
(new file)
Take Rewards + walls -->
Run opt agent -->
return opt_agent.reward

So GridworldMdp(GridworldMdpNoR):
	and from GMdp, you can call get_reward to get reward
	And this feeds into GridworldEnv

The question now is how gridworld mdps take in gridworlds...?
Okay, after looking at Gridworld_test.py, I realize that the grids that are used to initialze MDPs are 1 channel, and is {wall, space, reward}... 
okay, so extend agent class to include 2 grids:
	1 proxy
	1 true
	have 2 rewards:
	1 proxy
	1 true
	And also change get_mu_for_planning...
		Looking for example rn

so rn, VI agents want to exit when they encounter proxy reward, so I have to give it to them only in mus for planning, and have them blindly stumble on true reward...?

so write tests to make sure the walls + start of reward/proxy are the same
right now, spot 'A' appears 3 times in tests, and in the original walls passed in (mutative)
but I think I'm close
